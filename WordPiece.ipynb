{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f63852",
   "metadata": {},
   "source": [
    "# WordPiece Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd39ea",
   "metadata": {},
   "source": [
    "#### HADDOU Younes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0e5c1",
   "metadata": {},
   "source": [
    "This notebook presents the WordPiece tokenization algorithm used in BERT transformers.\n",
    "\n",
    "The goal is to create a WordPiece(text) function which takes as input a string representing the corpus and outputs the tokens vocabulary using the WordPiece Algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0436b",
   "metadata": {},
   "source": [
    "The algorithm can be defined as follows:\n",
    "\n",
    "    - Trivial word pre-tokenization of the corpus before\n",
    "    - Initial V0 vocabulary generation  \n",
    "    - As a first iteration, we associate to each word a list of it's chars (apple : [a, ##p, ##p, ##l, ##e]\n",
    "    - We simplify trough merges the chars list using a score method (score = freq_of_pairs/freq_first_element*freq_second_element)\n",
    "    - The end condition of the algorithm is the size of constituted vocabulary (size_vocab = size_V0 + simplified tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "597934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d082ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Trivial', 'word', 'pre-tokenization', 'of', 'the', 'corpus'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Trivial word pre-tokenization of the corpus'\n",
    "def basic_tokenize(text):\n",
    "  corpus = []\n",
    "\n",
    "  tokens = text.split()\n",
    "  token_counts = collections.Counter(tokens)\n",
    "\n",
    "\n",
    "  return token_counts\n",
    "\n",
    "count = basic_tokenize(text)\n",
    "count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4d7e5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##-',\n",
       " '##a',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##z',\n",
       " 'T',\n",
       " 'c',\n",
       " 'o',\n",
       " 'p',\n",
       " 't',\n",
       " 'w']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocab0(count):\n",
    "    \"\"\"\n",
    "    Generate a initial vocabulary \n",
    "    \"\"\"\n",
    "    v0 = []\n",
    "    for word in count.keys():\n",
    "        if word[0] not in v0:\n",
    "            v0.append(word[0])\n",
    "        for letter in word[1:]:\n",
    "            if f\"##{letter}\" not in v0:\n",
    "                v0.append(f\"##{letter}\")\n",
    "    v0.sort()\n",
    "    return v0\n",
    "\n",
    "vocab0(count)[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d012dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Trivial': ['T', '##r', '##i', '##v', '##i', '##a', '##l'],\n",
       " 'word': ['w', '##o', '##r', '##d'],\n",
       " 'pre-tokenization': ['p',\n",
       "  '##r',\n",
       "  '##e',\n",
       "  '##-',\n",
       "  '##t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'of': ['o', '##f'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'corpus': ['c', '##o', '##r', '##p', '##u', '##s']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_parts(count):\n",
    "    \"\"\"\n",
    "    Assotiate each words with its parts\n",
    "    \"\"\"\n",
    "    words_parts = {}\n",
    "    for word in count.keys():\n",
    "        parts=[]\n",
    "        for a,b in enumerate(word):\n",
    "            # words_parts[word] = b if a != 0 else f\"##{b}\"\n",
    "            parts.append(b if a == 0 else f\"##{b}\")\n",
    "        words_parts[word] = parts\n",
    "    return words_parts\n",
    "\n",
    "words_parts = get_words_parts(count)\n",
    "words_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c7f12ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('T', '##r'): 0.125,\n",
       " ('##r', '##i'): 0.015625,\n",
       " ('##i', '##v'): 0.0625,\n",
       " ('##v', '##i'): 0.0625,\n",
       " ('##i', '##a'): 0.03125,\n",
       " ('##a', '##l'): 0.25,\n",
       " ('w', '##o'): 0.125,\n",
       " ('##o', '##r'): 0.03125,\n",
       " ('##r', '##d'): 0.125,\n",
       " ('p', '##r'): 0.125,\n",
       " ('##r', '##e'): 0.025,\n",
       " ('##e', '##-'): 0.1,\n",
       " ('##-', '##t'): 0.125,\n",
       " ('##t', '##o'): 0.03125,\n",
       " ('##o', '##k'): 0.0625,\n",
       " ('##k', '##e'): 0.1,\n",
       " ('##e', '##n'): 0.06666666666666667,\n",
       " ('##n', '##i'): 0.041666666666666664,\n",
       " ('##i', '##z'): 0.0625,\n",
       " ('##z', '##a'): 0.125,\n",
       " ('##a', '##t'): 0.0625,\n",
       " ('##t', '##i'): 0.03125,\n",
       " ('##i', '##o'): 0.015625,\n",
       " ('##o', '##n'): 0.041666666666666664,\n",
       " ('o', '##f'): 1.0,\n",
       " ('t', '##h'): 0.5,\n",
       " ('##h', '##e'): 0.1,\n",
       " ('c', '##o'): 0.125,\n",
       " ('##r', '##p'): 0.0625,\n",
       " ('##p', '##u'): 0.25,\n",
       " ('##u', '##s'): 0.5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_score(parts, count):\n",
    "    \"\"\"\n",
    "    calculate scores for the paires\n",
    "    \"\"\"\n",
    "    letter_freqs = {}\n",
    "    pair_freqs = {}\n",
    "    for word, freq in count.items():\n",
    "        part = parts[word]\n",
    "        if len(part) == 1:\n",
    "            letter_freqs[part[0]] = letter_freqs.get(part[0], 0) + freq\n",
    "            continue\n",
    "        for i in range(len(part) - 1):\n",
    "            pair = (part[i], part[i + 1])\n",
    "            letter_freqs[pair[0]] = letter_freqs.get(pair[0], 0) + freq\n",
    "            letter_freqs[pair[1]] = letter_freqs.get(pair[1], 0) + freq\n",
    "            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
    "\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "scores = calculate_score(words_parts, count)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "750ba1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wo', '##r', '##d']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplify_parts(pair, words_parts):\n",
    "    \"\"\"\n",
    "    Simplify components list\n",
    "    \"\"\"\n",
    "    lc = pair[0]\n",
    "    rc = pair[1]\n",
    "    for word, part in words_parts.items():\n",
    "        if len(part) == 1:\n",
    "            continue\n",
    "        for i in range(len(part)-1):\n",
    "            if part[i] == lc and part[i + 1] == rc:\n",
    "                simplified = lc + rc[2:] if rc.startswith(\"##\") else lc + rc \n",
    "                part[i:i+2] = [simplified]  \n",
    "        words_parts[word] = part\n",
    "    return words_parts\n",
    "    \n",
    "simplify_parts(('w', '##o'),words_parts)\n",
    "words_parts['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26365903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w',\n",
       " 't',\n",
       " 's',\n",
       " 'rustl',\n",
       " 'rust',\n",
       " 'rus',\n",
       " 'ru',\n",
       " 'r',\n",
       " 'p',\n",
       " 'of',\n",
       " 'o',\n",
       " 'my',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'h',\n",
       " 'g',\n",
       " 'f',\n",
       " 'd',\n",
       " 'c',\n",
       " 'b',\n",
       " 'a',\n",
       " 'Th',\n",
       " 'T',\n",
       " 'I',\n",
       " 'Autum',\n",
       " 'Autu',\n",
       " 'Aut',\n",
       " 'Au',\n",
       " 'A',\n",
       " '##z',\n",
       " '##y',\n",
       " '##v',\n",
       " '##u',\n",
       " '##ty',\n",
       " '##t',\n",
       " '##s',\n",
       " '##r',\n",
       " '##q',\n",
       " '##p',\n",
       " '##o',\n",
       " '##n',\n",
       " '##m',\n",
       " '##lity',\n",
       " '##l',\n",
       " '##k',\n",
       " '##ity',\n",
       " '##i',\n",
       " '##h',\n",
       " '##g',\n",
       " '##f',\n",
       " '##e',\n",
       " '##d',\n",
       " '##c',\n",
       " '##a',\n",
       " '##.',\n",
       " '##,']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 57\n",
    "\n",
    "def WordPiece(text)->list:\n",
    "    \"\"\"\n",
    "    Aggregate all above functions.\n",
    "    Using VOCAB_SIZE, the vocabulary's granularity can be defined\n",
    "    \"\"\"\n",
    "    count = basic_tokenize(text)\n",
    "    v0 = vocab0(count)\n",
    "    words_parts = get_words_parts(count)\n",
    "    while len(v0) < VOCAB_SIZE:\n",
    "        scores = calculate_score(words_parts, count)\n",
    "        # This try block catch an error that happens when VOCAB_SIZE is too high compared to the corpus\n",
    "        try:\n",
    "            best_pair = max(scores, key=scores.get)\n",
    "        except ValueError:\n",
    "            print(f\"VOCAB_SIZE is to high, maximum VOCAB_SIZE is:{len(v0)}\")\n",
    "            break\n",
    "        lc = best_pair[0]\n",
    "        rc = best_pair[1]\n",
    "        max_score = scores[best_pair]\n",
    "        words_parts = simplify_parts(best_pair, words_parts)\n",
    "        tokened_part =  lc + rc[2:] if rc.startswith(\"##\") else lc + rc \n",
    "        v0.append(tokened_part)\n",
    "    v0.sort(reverse=True)\n",
    "    return v0\n",
    "    \n",
    "    \n",
    "WordPiece(\"Autumn leaves rustled in the gentle breeze as I walked through the peaceful forest. The golden hues of the trees created a mesmerizing landscape, and the sound of a distant stream added to the tranquility of the scene. I stopped to take a deep breath, savoring the crisp, fresh air before continuing my hike.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7347f",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51232bb5",
   "metadata": {},
   "source": [
    "The pre-tokenization (whitespace tokenization) here is very simple and does not take into account punctuation and other special characters. So some tokens may be unaccurate. To solve this problem we can use a nlp pre-tokenization model. I did not want to use external tools like Spacy in order to stay within the framework of the exercise.\n",
    "\n",
    "Finally, to tokenize a text we apply the algoritm on each word and we take the biggest subword present in the generated vocabulary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
