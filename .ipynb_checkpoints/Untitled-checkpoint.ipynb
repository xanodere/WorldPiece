{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be0551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1c869fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Le petit Renard\n",
    "\n",
    "Il était une fois un petit renard roux qui vivait seul dans la forêt. Il aimait jouer et courir dans les bois, mais parfois il se sentait un peu seul. Un jour, en se promenant, il croisa un vieux loup gris. \"Bonjour petit renard, que fais-tu tout seul?\", demanda le loup.\n",
    "\n",
    "\"Je m'ennuie un peu, tous mes amis sont partis et je n'ai personne avec qui jouer\", répondit le renard tristement. \"Pourquoi n'irais-tu pas voir les lapins dans la clairière, je suis sûr qu'ils accepteront de jouer avec toi\", suggéra le loup. Le renard remercia le loup et se dirigea joyeusement vers la clairière.\n",
    "\n",
    "Lorsqu'il arriva, il vit plusieurs lapins qui sautaient et couraient. \"Bonjour les amis, est-ce que je peux jouer avec vous?\", leur demanda le petit renard timidement. \"Bien sûr!\", répondirent les lapins. Et ils passèrent tout l'après-midi à jouer ensemble dans la clairière. Le petit renard n'était plus seul et avait enfin trouvé de nouveaux amis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4338dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def basique_tokenize(text):\n",
    "  corpus = []\n",
    "\n",
    "  tokens = text.split()\n",
    "  token_counts = collections.Counter(tokens)\n",
    "\n",
    "\n",
    "  return token_counts\n",
    "\n",
    "count = basique_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "550def69",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Counter' object has no attribute 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcount\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Counter' object has no attribute 'key'"
     ]
    }
   ],
   "source": [
    "count.key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34a722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10\n",
    "def word_piece(corpus, vocab):\n",
    "\n",
    "  # Calcul des fréquences de chaque élément\n",
    "  counts = collections.Counter([item for sublist in corpus for item in sublist])  \n",
    "  print(counts)\n",
    "  # Boucle sur les merges\n",
    "  while len(vocab) < max_vocab_size:\n",
    "\n",
    "    # Calcul du score pour tous les pairs possibles\n",
    "    pairs = []\n",
    "    for pair in get_all_pairs(vocab):\n",
    "      score = get_pair_score(pair, counts)\n",
    "      pairs.append((pair, score))\n",
    "\n",
    "    # Sélection du meilleur pair à merger\n",
    "    best_pair = max(pairs, key=lambda x: x[1])\n",
    "    pair, score = best_pair\n",
    "\n",
    "    # Merge du pair\n",
    "    new_token = merge_pair(pair)  \n",
    "    vocab.append(new_token)\n",
    "\n",
    "    # Mise à jour du corpus et des fréquences\n",
    "    corpus = apply_merge(corpus, pair, new_token)\n",
    "    counts = update_counts(counts, pair, new_token)\n",
    "\n",
    "  return corpus, vocab\n",
    "\n",
    "intial_vocab, initial_corpus = basique_tokenize(text)\n",
    "corpus, vocab = word_piece(initial_corpus,intial_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15173252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'Le',\n",
       " 'petit',\n",
       " 'Renard',\n",
       " '\\n\\n',\n",
       " 'Il',\n",
       " 'était',\n",
       " 'une',\n",
       " 'fois',\n",
       " 'un',\n",
       " 'petit',\n",
       " 'renard',\n",
       " 'roux',\n",
       " 'qui',\n",
       " 'vivait',\n",
       " 'seul',\n",
       " 'dans',\n",
       " 'la',\n",
       " 'forêt',\n",
       " '.',\n",
       " 'Il',\n",
       " 'aimait',\n",
       " 'jouer',\n",
       " 'et',\n",
       " 'courir',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'bois',\n",
       " ',',\n",
       " 'mais',\n",
       " 'parfois',\n",
       " 'il',\n",
       " 'se',\n",
       " 'sentait',\n",
       " 'un',\n",
       " 'peu',\n",
       " 'seul',\n",
       " '.',\n",
       " 'Un',\n",
       " 'jour',\n",
       " ',',\n",
       " 'en',\n",
       " 'se',\n",
       " 'promenant',\n",
       " ',',\n",
       " 'il',\n",
       " 'croisa',\n",
       " 'un',\n",
       " 'vieux',\n",
       " 'loup',\n",
       " 'gris',\n",
       " '.',\n",
       " '\"',\n",
       " 'Bonjour',\n",
       " 'petit',\n",
       " 'renard',\n",
       " ',',\n",
       " 'que',\n",
       " 'fais',\n",
       " '-',\n",
       " 'tu',\n",
       " 'tout',\n",
       " 'seul',\n",
       " '?',\n",
       " '\"',\n",
       " ',',\n",
       " 'demanda',\n",
       " 'le',\n",
       " 'loup',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " '\"',\n",
       " 'Je',\n",
       " \"m'\",\n",
       " 'ennuie',\n",
       " 'un',\n",
       " 'peu',\n",
       " ',',\n",
       " 'tous',\n",
       " 'mes',\n",
       " 'amis',\n",
       " 'sont',\n",
       " 'partis',\n",
       " 'et',\n",
       " 'je',\n",
       " \"n'\",\n",
       " 'ai',\n",
       " 'personne',\n",
       " 'avec',\n",
       " 'qui',\n",
       " 'jouer',\n",
       " '\"',\n",
       " ',',\n",
       " 'répondit',\n",
       " 'le',\n",
       " 'renard',\n",
       " 'tristement',\n",
       " '.',\n",
       " '\"',\n",
       " 'Pourquoi',\n",
       " \"n'\",\n",
       " 'irais',\n",
       " '-',\n",
       " 'tu',\n",
       " 'pas',\n",
       " 'voir',\n",
       " 'les',\n",
       " 'lapins',\n",
       " 'dans',\n",
       " 'la',\n",
       " 'clairière',\n",
       " ',',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'sûr',\n",
       " \"qu'\",\n",
       " 'ils',\n",
       " 'accepteront',\n",
       " 'de',\n",
       " 'jouer',\n",
       " 'avec',\n",
       " 'toi',\n",
       " '\"',\n",
       " ',',\n",
       " 'suggéra',\n",
       " 'le',\n",
       " 'loup',\n",
       " '.',\n",
       " 'Le',\n",
       " 'renard',\n",
       " 'remercia',\n",
       " 'le',\n",
       " 'loup',\n",
       " 'et',\n",
       " 'se',\n",
       " 'dirigea',\n",
       " 'joyeusement',\n",
       " 'vers',\n",
       " 'la',\n",
       " 'clairière',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " \"Lorsqu'\",\n",
       " 'il',\n",
       " 'arriva',\n",
       " ',',\n",
       " 'il',\n",
       " 'vit',\n",
       " 'plusieurs',\n",
       " 'lapins',\n",
       " 'qui',\n",
       " 'sautaient',\n",
       " 'et',\n",
       " 'couraient',\n",
       " '.',\n",
       " '\"',\n",
       " 'Bonjour',\n",
       " 'les',\n",
       " 'amis',\n",
       " ',',\n",
       " 'est',\n",
       " '-ce',\n",
       " 'que',\n",
       " 'je',\n",
       " 'peux',\n",
       " 'jouer',\n",
       " 'avec',\n",
       " 'vous',\n",
       " '?',\n",
       " '\"',\n",
       " ',',\n",
       " 'leur',\n",
       " 'demanda',\n",
       " 'le',\n",
       " 'petit',\n",
       " 'renard',\n",
       " 'timidement',\n",
       " '.',\n",
       " '\"',\n",
       " 'Bien',\n",
       " 'sûr',\n",
       " '!',\n",
       " '\"',\n",
       " ',',\n",
       " 'répondirent',\n",
       " 'les',\n",
       " 'lapins',\n",
       " '.',\n",
       " 'Et',\n",
       " 'ils',\n",
       " 'passèrent',\n",
       " 'tout',\n",
       " \"l'\",\n",
       " 'après-midi',\n",
       " 'à',\n",
       " 'jouer',\n",
       " 'ensemble',\n",
       " 'dans',\n",
       " 'la',\n",
       " 'clairière',\n",
       " '.',\n",
       " 'Le',\n",
       " 'petit',\n",
       " 'renard',\n",
       " \"n'\",\n",
       " 'était',\n",
       " 'plus',\n",
       " 'seul',\n",
       " 'et',\n",
       " 'avait',\n",
       " 'enfin',\n",
       " 'trouvé',\n",
       " 'de',\n",
       " 'nouveaux',\n",
       " 'amis',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cafc0fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'Le',\n",
       " 'petit',\n",
       " '\\n\\n',\n",
       " 'Il',\n",
       " 'était',\n",
       " 'un',\n",
       " 'renard',\n",
       " 'qui',\n",
       " 'seul',\n",
       " 'dans',\n",
       " 'la',\n",
       " '.',\n",
       " 'jouer',\n",
       " 'et',\n",
       " 'les',\n",
       " ',',\n",
       " 'il',\n",
       " 'se',\n",
       " 'peu',\n",
       " 'loup',\n",
       " '\"',\n",
       " 'Bonjour',\n",
       " 'que',\n",
       " '-',\n",
       " 'tu',\n",
       " 'tout',\n",
       " '?',\n",
       " 'demanda',\n",
       " 'le',\n",
       " 'amis',\n",
       " 'je',\n",
       " \"n'\",\n",
       " 'avec',\n",
       " 'lapins',\n",
       " 'clairière',\n",
       " 'sûr',\n",
       " 'ils',\n",
       " 'de']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d5bc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def wordpiece_tokenize(word, vocab):\n",
    "    tokenization = []\n",
    "    while len(word) > 0:\n",
    "        found = False\n",
    "        for i in range(len(word), 0, -1):\n",
    "            subword = word[:i]\n",
    "            if subword in vocab:\n",
    "                tokenization.append(subword)\n",
    "                word = word[i:]\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            tokenization.append(word)\n",
    "            break\n",
    "    return tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "243bec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'Le', 'petit', 'Renard', '\\n\\n', 'Il', 'était', 'un', 'e', 'fois']\n",
      "Vocab size: 50\n",
      "['\\n', 'Le', 'petit', 'Renard', '\\n\\n', 'Il', 'était', 'un', 'e', 'fois']\n",
      "Vocab size: 50\n",
      "['\\n', 'Le', 'petit', 'Renard', '\\n\\n', 'Il', 'était', 'un', 'e', 'fois']\n",
      "Vocab size: 50\n",
      "['\\n', 'Le', 'petit', 'Renard', '\\n\\n', 'Il', 'était', 'un', 'e', 'fois']\n",
      "Vocab size: 50\n",
      "['\\n', 'Le', 'petit', 'Renard', '\\n\\n', 'Il', 'était', 'un', 'e', 'fois']\n",
      "Vocab size: 50\n"
     ]
    }
   ],
   "source": [
    " \n",
    "for i in range(NUM_ITERATIONS):\n",
    "\n",
    "  # Tokenize avec le vocab actuel\n",
    "  subtokens = []\n",
    "  for word in tokens:\n",
    "    subtokens.extend(wordpiece_tokenize(word, vocab))  \n",
    "  print(subtokens[0:10])\n",
    "  # Compte les suffixes  \n",
    "  subcounts = Counter(subtokens)\n",
    "\n",
    "  # Ajoute les plus fréquents\n",
    "  for suffix, count in subcounts.most_common(TOP_K):\n",
    "    if suffix not in vocab:\n",
    "      vocab.append(suffix)\n",
    "      \n",
    "  print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "# Sauvegarde le vocab final\n",
    "with open('vocab.txt', 'w') as f:\n",
    "  for word in vocab:\n",
    "    f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53db5036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Vocab size: 50, Vocab: ['\\n', 'Le', 'petit', '\\n\\n', 'Il', 'était', 'un', 'renard', 'qui', 'seul']\n",
      "Iteration 2: Vocab size: 50, Vocab: ['\\n', 'Le', 'petit', '\\n\\n', 'Il', 'était', 'un', 'renard', 'qui', 'seul']\n",
      "Iteration 3: Vocab size: 50, Vocab: ['\\n', 'Le', 'petit', '\\n\\n', 'Il', 'était', 'un', 'renard', 'qui', 'seul']\n",
      "Iteration 4: Vocab size: 50, Vocab: ['\\n', 'Le', 'petit', '\\n\\n', 'Il', 'était', 'un', 'renard', 'qui', 'seul']\n",
      "Iteration 5: Vocab size: 50, Vocab: ['\\n', 'Le', 'petit', '\\n\\n', 'Il', 'était', 'un', 'renard', 'qui', 'seul']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Function to tokenize a word using WordPiece-like tokenization\n",
    "def wordpiece_tokenize(word, vocab):\n",
    "    tokenization = []\n",
    "    while len(word) > 0:\n",
    "        found = False\n",
    "        for i in range(len(word), 0, -1):\n",
    "            subword = word[:i]\n",
    "            if subword in vocab:\n",
    "                tokenization.append(subword)\n",
    "                word = word[i:]\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            tokenization.append(word)\n",
    "            break\n",
    "    return tokenization\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Parameters\n",
    "MIN_COUNT = 2\n",
    "TOP_K = 50\n",
    "NUM_ITERATIONS = 5\n",
    "VOCAB_SIZE = 1000  # Define the desired vocabulary size\n",
    "\n",
    "# Tokenize and count the text\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "counts = Counter(tokens)\n",
    "\n",
    "# Initial vocabulary (V0) from spaCy\n",
    "vocab = [word for word, count in counts.items() if count >= MIN_COUNT]\n",
    "\n",
    "# Iteratively build the WordPiece-like vocabulary\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    subtokens = []\n",
    "    for word in tokens:\n",
    "        subtokens.extend(wordpiece_tokenize(word, vocab))\n",
    "    \n",
    "    subcounts = Counter(subtokens)\n",
    "    \n",
    "    # Add the most frequent subtokens to the vocabulary\n",
    "    for suffix, count in subcounts.most_common(TOP_K):\n",
    "        if suffix not in vocab:\n",
    "            vocab.append(suffix)\n",
    "    \n",
    "    print(f\"Iteration {i + 1}: Vocab size: {len(vocab)}, Vocab:{vocab[0:10]}\")\n",
    "\n",
    "    # Check if the desired vocabulary size is reached\n",
    "    if len(vocab) >= VOCAB_SIZE:\n",
    "        break\n",
    "\n",
    "# Save the final vocabulary to a file\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5b8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
